import sqlite3
import json
import pandas as pd
from datetime import datetime
import sys
import os

# Add the parent directory to sys.path to import from models
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from models.scrape_mining import MiningScraper


def create_news_table(conn: sqlite3.Connection):
    """
    Create mining_news table if it doesn't exist.
    """
    conn.execute(
        """
    CREATE TABLE IF NOT EXISTS mining_news (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT NOT NULL,
        body TEXT,
        source TEXT,
        timestamp TEXT,
        commodities TEXT,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(source)
    );
    """
    )
    conn.commit()


def load_news_data(json_path: str) -> list:
    """
    Load news data from JSON file generated by scrape_mining.py
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"File {json_path} not found.")
        return []
    except json.JSONDecodeError:
        print(f"Invalid JSON in {json_path}")
        return []


def prepare_news_data(articles: list) -> pd.DataFrame:
    """
    Prepare articles data for database insertion.
    Convert commodities list to comma-separated string.
    """
    if not articles:
        return pd.DataFrame()
    
    df = pd.DataFrame(articles)
    
    # Convert commodities list to comma-separated string
    df['commodities'] = df['commodities'].apply(
        lambda x: ', '.join(x) if isinstance(x, list) and x else ''
    )
    
    # Filter out articles with missing critical data
    required_cols = ['title']
    df = df.dropna(subset=required_cols)
    
    # Remove articles with empty titles
    df = df[df['title'].str.strip() != '']
    
    return df


def insert_news_records(conn: sqlite3.Connection, df: pd.DataFrame):
    """
    Insert news records into mining_news table.
    """
    if df.empty:
        print("No valid news records to insert.")
        return

    insert_sql = """
    INSERT OR IGNORE INTO mining_news (
        title, body, source, timestamp, commodities
    ) VALUES (
        :title, :body, :source, :timestamp, :commodities
    );
    """

    records = df[['title', 'body', 'source', 'timestamp', 'commodities']].to_dict(orient='records')

    # Count rows before insert
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM mining_news;")
    before = cur.fetchone()[0]

    with conn:
        conn.executemany(insert_sql, records)

    # Count rows after insert
    cur.execute("SELECT COUNT(*) FROM mining_news;")
    after = cur.fetchone()[0]

    inserted = after - before
    print(f"Inserted {inserted} new news records (duplicates ignored).")


def scrape_and_insert_news(num_pages: int, db_path: str, output_filename: str = None):
    """
    Full pipeline:
    1. Scrape news data using MiningScraper
    2. Save to JSON (optional)
    3. Prepare data for database
    4. Create table if needed
    5. Insert into SQLite
    """
    # Step 1: Scrape news data
    print(f"Scraping {num_pages} pages of mining news...")
    scraper = MiningScraper()
    articles = scraper.extract_news_pages(num_pages)
    
    if not articles:
        print("No articles found.")
        return
    
    print(f"Scraped {len(articles)} articles.")
    
    # Step 2: Save to JSON (optional)
    if output_filename:
        scraper.write_json(articles, output_filename)
        print(f"Saved articles to {output_filename}.json")
    
    # Step 3: Prepare data
    df = prepare_news_data(articles)
    
    if df.empty:
        print("No valid articles to insert after filtering.")
        return
    
    # Step 4 & 5: Create table and insert data
    conn = sqlite3.connect(db_path)
    create_news_table(conn)
    insert_news_records(conn, df)
    conn.close()


def load_and_insert_news(json_path: str, db_path: str):
    """
    Load news data from existing JSON file and insert into database.
    """
    print(f"Loading news data from {json_path}...")
    articles = load_news_data(json_path)
    
    if not articles:
        return
    
    print(f"Loaded {len(articles)} articles.")
    
    # Prepare data
    df = prepare_news_data(articles)
    
    if df.empty:
        print("No valid articles to insert after filtering.")
        return
    
    # Create table and insert data
    conn = sqlite3.connect(db_path)
    create_news_table(conn)
    insert_news_records(conn, df)
    conn.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Pipeline for inserting mining news into database")
    parser.add_argument("--scrape", action="store_true", help="Scrape new data from mining.com")
    parser.add_argument("--load", type=str, help="Load data from existing JSON file")
    parser.add_argument("--pages", type=int, default=1, help="Number of pages to scrape (default: 1)")
    parser.add_argument("--db", type=str, default="db.sqlite", help="Database path (default: db.sqlite)")
    parser.add_argument("--output", type=str, help="Output filename for JSON (optional)")
    
    args = parser.parse_args()
    
    if args.scrape:
        scrape_and_insert_news(args.pages, args.db, args.output)
    elif args.load:
        load_and_insert_news(args.load, args.db)
    else:
        print("Please specify either --scrape or --load option.")
        print("Examples:")
        print("  python pipeline.py --scrape --pages 3 --db db.sqlite --output mining_news")
        print("  python pipeline.py --load mining_news.json --db db.sqlite")