from datetime import datetime, timedelta

from insider_news.models.scrape_coalmetal    import run_coalmetal_scraping
from insider_news.models.scrape_mining       import MiningScraper
from insider_news.models.scrape_ima          import IMANewsScraper
from insider_news.models.scrape_nikel        import NikelCoIdScraper
from insider_news.models.scrape_ruang_energi import RuangEnergiScraper
from insider_news.base_model.scraper         import ScraperCollection

import sqlite3
import json
import pandas as pd
import os
import logging 
import argparse


logging.basicConfig(
    level=logging.INFO, # Set the logging level
    format='%(asctime)s [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
    )

LOGGER = logging.getLogger(__name__)
LOGGER.info("Init Global Variable")


def get_connection(db_path: str = 'db.sqlite') -> sqlite3.Connection:
    """
    Create a SQLite connection to the specified database.
    
    Args:
        db_path: Path to the SQLite database file
    
    Returns:
        sqlite3.Connection: Connection object to the SQLite database
    """
    try:
        conn = sqlite3.connect(db_path)
        LOGGER.info(f"Connected to database at {db_path}")
        return conn
    except sqlite3.Error as e:
        LOGGER.error(f"Error connecting to database: {e}")
        raise
    

def create_news_table(conn: sqlite3.Connection):
    """
    Create mining_news table if it doesn't exist.
    
    Args:
        conn: SQLite connection object
    """
    conn.execute(
        """
    CREATE TABLE IF NOT EXISTS mining_news (
        id INTEGER PRIMARY KEY, 
        title TEXT NOT NULL,
        body TEXT,
        source TEXT,
        timestamp TEXT,
        commodities TEXT,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(source)
    );
    """
    )
    conn.commit()


def get_next_id(conn: sqlite3.Connection) -> int:
    """
    Get the next available ID by finding the maximum existing ID.
    
    Args:
        conn: SQLite connection object
    
    Returns:
        int: Next available ID for insertion
    """
    cur = conn.cursor()
    cur.execute("SELECT MAX(id) FROM mining_news;")
    result = cur.fetchone()[0]
    return (result + 1) if result is not None else 1


def load_news_data(json_path: str) -> list:
    """
    Load news data from JSON file generated by scrape_mining.py
    
    Args:
        json_path: Path to the JSON file containing news articles
    
    Returns:
        list: List of articles loaded from the JSON file
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as file:
            return json.load(file)
    except FileNotFoundError:
        LOGGER.info(f"File {json_path} not found.")
        return []
    except json.JSONDecodeError:
        LOGGER.info(f"Invalid JSON in {json_path}")
        return []


def prepare_news_data(articles: list) -> pd.DataFrame:
    """
    Prepare articles data for database insertion.
    Convert commodities list to comma-separated string.
    
    Args:
        articles: List of articles, each article is a dictionary
        
    Returns:
        pd.DataFrame: DataFrame containing prepared articles data
    """
    if not articles:
        return pd.DataFrame()
    
    df = pd.DataFrame(articles)
    
    # Filter out articles with missing critical data
    required_cols = ['title']
    df = df.dropna(subset=required_cols)
    
    # Remove articles with empty titles
    df = df[df['title'].str.strip() != '']

    return df 


def filter_new_articles(conn: sqlite3.Connection, df:pd.DataFrame) -> pd.DataFrame: 
    """ 
    Filter out articles that already exist in the database to prevent duplicates.
    
    Args: 
        conn: SQLite connection object
        df: DataFrame containing articles to be inserted
    
    Returns:
        pd.DataFrame: DataFrame containing only new articles that are not in the database
    """
    if df.empty: 
        return df 
    
    df['source'] = df['source'].str.strip().str.lower()
    # Remove duplicates within the current DataFrame
    df = df.drop_duplicates(subset=['source'], keep='first')

    # Get existing sources from the database
    cursor = conn.cursor()
    cursor.execute("SELECT source FROM mining_news WHERE source IS NOT NULL;")
    existing_sources = set(row[0] for row in cursor.fetchall())
    
    # Filter out articles that already exist in the database
    new_articles = df[~df['source'].isin(existing_sources)]
    if not new_articles.empty:
        LOGGER.warning(f"Attempted to insert: {new_articles['source'].tolist()}")
    
    LOGGER.info(f"Found {len(df)} total articles, {len(new_articles)} are new.")
    return new_articles


def insert_news_records(conn: sqlite3.Connection, df: pd.DataFrame):
    """
    Insert news records into mining_news table. 
    Check for duplicates based on source and construct a new ID for each article. 
    Only insert new articles.
    
    Args:
        conn: SQLite connection object
        df: DataFrame containing articles to be inserted
    """
    # Check if DataFrame is empty
    if df.empty:
        LOGGER.info("No valid news records to insert.")
        return

    # Filter out new articles that are not already in the database
    new_articles_df = filter_new_articles(conn, df)
    
    # If no new articles, exit early
    if new_articles_df.empty:
        LOGGER.info("No new articles to insert (all are duplicates).")
        return
    
    # Get the next available ID for insertion
    next_id = get_next_id(conn)
    
    # Assign consecutive IDs to new articles
    new_articles_df = new_articles_df.copy()
    new_articles_df['id'] = range(next_id, next_id + len(new_articles_df))

    # Prepare SQL insert statement
    insert_sql = """
    INSERT INTO mining_news (
        id, title, body, source, timestamp, commodities
    ) VALUES (
        :id, :title, :body, :source, :timestamp, :commodities
    );
    """
    
    # Convert commodities list to JSON string
    new_articles_df['commodities'] = new_articles_df['commodities'].apply(
        lambda x: json.dumps(x) if isinstance(x, list) else []
    )
    
    # Convert DataFrame to list of dictionaries for insertion
    records = new_articles_df[['id', 'title', 'body', 'source', 'timestamp', 'commodities']].to_dict(orient='records')
    
    # Count rows before insert
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM mining_news;")
    before = cur.fetchone()[0]

    with conn:
        conn.executemany(insert_sql, records)

    # Count rows after insert
    cur.execute("SELECT COUNT(*) FROM mining_news;")
    after = cur.fetchone()[0]

    inserted = after - before
    LOGGER.info(f"Inserted {inserted} new news records (duplicates ignored).")


def scrape_and_insert_daily_news(num_pages: int, db_path: str, output_filename: str = None):
    """
    Pipeline to scrape daily news from multiple sources and insert into database.

    Args:
        num_pages: Number of pages to scrape from each source
        db_path: Path to SQLite database
        output_filename: Optional filename to save scraped articles as JSON
    """
    LOGGER.info(f"Scraping {num_pages} pages of every news sources...")

    scraper_mining = MiningScraper()
    scraper_nikel = NikelCoIdScraper()
    scraper_ima = IMANewsScraper()
    scraper_ruangenergi = RuangEnergiScraper()

    # Add scrapper function to scraper collection list
    scraper_collection = ScraperCollection()
    scraper_collection.add_scraper(scraper_mining)
    scraper_collection.add_scraper(scraper_nikel)
    scraper_collection.add_scraper(scraper_ima)
    scraper_collection.add_scraper(scraper_ruangenergi)

    # Run scraper
    article_lists = scraper_collection.run_all(num_pages)
    LOGGER.info(f"Scraped {len(article_lists)} articles.")
    
    if output_filename:
        os.makedirs('insider_news/data', exist_ok=True)
        scraper_collection.write_json(article_lists, output_filename)
        LOGGER.info(f"Saved articles to {output_filename}.json")
    
    df = prepare_news_data(article_lists)
    
    if df.empty:
        LOGGER.info("No valid articles to insert after filtering.")
        return
    
    conn = get_connection(db_path)
    create_news_table(conn)
    insert_news_records(conn, df)
    conn.close()


def scrape_and_insert_coalmetal_news(limit_articles: int, db_path: str, 
                                     initial_run: bool, score_limit: int): 
    """ 
    Full pipeline for scraping coalmetal.com and inserting into local database.

    Args:
        limit_articles: Limit the number of articles to scrape from coalmetal.com
        db_path: Path to SQLite database
        initial_run: If True, will only keep top 15 articles based on score
        score_limit: Minimum score to filter articles from coalmetal.com
    """
    df_articles_coalmetal = run_coalmetal_scraping(initial_run=initial_run, limit_articles=limit_articles)
    if df_articles_coalmetal.empty:
        LOGGER.info("No articles found from coalmetal.com")
        return pd.DataFrame()
    
    LOGGER.info(f"Scraped {df_articles_coalmetal.shape[0]} articles from coalmetal")

    df_articles_coalmetal = df_articles_coalmetal[df_articles_coalmetal['score'] > score_limit]
    
    if initial_run:
        df_articles_coalmetal = df_articles_coalmetal.sort_values("score", ascending=False).head(15).copy()
        LOGGER.info(f"Initial run: inserting only top 15 articles with score > {score_limit}")

    conn = get_connection(db_path)
    create_news_table(conn)

    #insert for coalmetal scraping
    insert_news_records(conn, df_articles_coalmetal)

    conn.close()


def load_and_insert_news(json_path: str):
    """
    Load news data from existing JSON file and insert into database.
    
    Args: 
        json_path: Path to the JSON file containing news articles
        db_path: Path to SQLite database
    """
    LOGGER.info(f"Loading news data from {json_path}...")
    articles = load_news_data(json_path)
    
    if not articles:
        return
    
    LOGGER.info(f"Loaded {len(articles)} articles.")
    
    # Prepare data
    df = prepare_news_data(articles)
    
    if df.empty:
        LOGGER.info("No valid articles to insert after filtering.")
        return
    
    # Create table and insert data
    conn = get_connection()
    create_news_table(conn)
    insert_news_records(conn, df)
    conn.close()

    
def archive_old_news(days_old: int = 182, archive_path: str = 'insider_news/data/archive'): 
    """
    Delete news articles older than specified days based on their publication timestamp.
    
    Args:
        db_path: Path to SQLite database
        days_old: Number of days to consider for archiving old articles
    """
    # Create a connection to the database
    conn = get_connection()

    # Calculate cutoff date
    cutoff_date = datetime.now() - timedelta(days=days_old)
    cutoff_str = cutoff_date.strftime('%Y-%m-%d')
    
    # SQL to delete old articles
    query = """
        SELECT * FROM mining_news 
        WHERE timestamp IS NOT NULL 
        AND timestamp < ?
    """
    
    # Execute deletion
    articles_to_archive_df  = pd.read_sql_query(query, conn, params=(cutoff_str,))
    conn.close()
    
    if articles_to_archive_df.empty:
        LOGGER.info(f"No articles older than {days_old} days found to archive.")
        conn.close()
        return pd.DataFrame()
    
    # Create archive directory if it doesn't exist
    os.makedirs(archive_path, exist_ok=True)
    csv_path = os.path.join(archive_path, f'mining_news_all_archived.csv')
    
    # Check if the archive file already exists and read existing sources
    existing_archived_sources = set()
    if os.path.exists(csv_path):
        try: 
            # Read the unique 'source' column from the existing archive
            archive_df = pd.read_csv(csv_path, usecols=['source'])
            existing_archived_sources = set(archive_df['source'])
        except (pd.errors.EmptyDataError, FileNotFoundError):
            LOGGER.error(f"Archive file at {csv_path} is empty or not found. A new one will be created.")
        except Exception as e:
            LOGGER.error(f"Could not read existing archive file: {e}")
            return
    
    # Filter out articles that have already been archived
    unique_articles_to_archive_df = articles_to_archive_df[
        ~articles_to_archive_df['source'].isin(existing_archived_sources)
    ]
    
    # If no new articles to archive, log and exit
    if unique_articles_to_archive_df.empty:
        LOGGER.info("All old articles found in the database have already been archived.")
        return
    
    # Add archived date column
    unique_articles_to_archive_df = unique_articles_to_archive_df.copy()
    unique_articles_to_archive_df['archived_date'] = datetime.now().strftime('%Y-%m-%d')
    
    # Check if the file is empty to decide whether to write headers
    file_exists_and_is_not_empty = os.path.exists(csv_path) and os.path.getsize(csv_path) > 0

    # Write the unique articles to the CSV file, appending if it exists
    unique_articles_to_archive_df.to_csv(
        csv_path, 
        mode='a', 
        header=not file_exists_and_is_not_empty, 
        index=False
    )

    LOGGER.info(f"Archived {len(unique_articles_to_archive_df)} new articles to {csv_path}. Duplicates were ignored.")
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipeline for inserting mining news into database")
    parser.add_argument("--scrape-news", action="store_true", help="Scrape new data from mining.com")
    parser.add_argument("--load", type=str, help="Load data from existing JSON file")
    parser.add_argument("--pages", type=int, default=1, help="Number of pages to scrape (default: 1)")
    parser.add_argument("--db", type=str, default="db.sqlite", help="Database path (default: db.sqlite)")
    parser.add_argument("--output", type=str, help="Output filename for JSON (optional)")
    parser.add_argument("--scrape-coalmetal", action="store_true", help="Only scrape coalmetal.com")
    parser.add_argument("--limit-coalmetal", type=int, default=15, help="Limit number of articles to scrape coalmetal.com (default: 10)")
    parser.add_argument("--initial-run", action="store_true", default=False, help="Set initial run for coalmetal scraping (default: False)")
    parser.add_argument("--minimum-score", type=int, default=65, help="Score limit for filtering coalmetal articles (default: 65)")
    
    parser.add_argument("--archive", action="store_true", help="Archive all mining_news records older than `--days-old` days into CSV")
    parser.add_argument("--days-old", type=int, default=182, dest="days_old", help="Archive articles older than this many days (default: 182)")
    parser.add_argument("--archive-path", type=str, default="insider_news/data/archive", dest="archive_path", help="Path to the directory for saving archived news")

    args = parser.parse_args()
    
    if args.scrape_news:
        scrape_and_insert_daily_news(args.pages, args.db, args.output)
    elif args.scrape_coalmetal:
        scrape_and_insert_coalmetal_news(args.limit_coalmetal, args.db, args.initial_run, args.minimum_score)
    elif args.load:
        load_and_insert_news(args.load, args.db)
    elif args.archive: 
        archive_old_news(days_old=args.days_old, archive_path=args.archive_path)
    else:
        LOGGER.info("Please specify either --scrape-news, --scrape-coalmetal, --load, or --archive option.")
        LOGGER.info("Examples:")
        LOGGER.info("  python pipeline.py --scrape-news --pages 3 --db db.sqlite --output mining_news")
        LOGGER.info("  python pipeline.py --load insider_news/data/mining_news.json --db db.sqlite")