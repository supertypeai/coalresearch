from datetime import datetime, timedelta
from typing import Optional

from insider_news.models.scrape_coalmetal import run_coalmetal_scraping
from insider_news.models.scrape_mining import MiningScraper

import sqlite3
import json
import pandas as pd
import sys
import os
import logging 
import argparse

logging.basicConfig(
    level=logging.INFO, # Set the logging level
    format='%(asctime)s [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
    )

LOGGER = logging.getLogger(__name__)
LOGGER.info("Init Global Variable")


def get_connection(db_path: str = 'db.sqlite') -> sqlite3.Connection:
    """
    Create a SQLite connection to the specified database.
    
    Args:
        db_path: Path to the SQLite database file
    
    Returns:
        sqlite3.Connection: Connection object to the SQLite database
    """
    try:
        conn = sqlite3.connect(db_path)
        LOGGER.info(f"Connected to database at {db_path}")
        return conn
    except sqlite3.Error as e:
        LOGGER.error(f"Error connecting to database: {e}")
        raise
    

def create_news_table(conn: sqlite3.Connection):
    """
    Create mining_news table if it doesn't exist.
    
    Args:
        conn: SQLite connection object
    """
    conn.execute(
        """
    CREATE TABLE IF NOT EXISTS mining_news (
        id INTEGER PRIMARY KEY, 
        title TEXT NOT NULL,
        body TEXT,
        source TEXT,
        timestamp TEXT,
        commodities TEXT,
        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(source)
    );
    """
    )
    conn.commit()


def get_next_id(conn: sqlite3.Connection) -> int:
    """
    Get the next available ID by finding the maximum existing ID.
    
    Args:
        conn: SQLite connection object
    
    Returns:
        int: Next available ID for insertion
    """
    cur = conn.cursor()
    cur.execute("SELECT MAX(id) FROM mining_news;")
    result = cur.fetchone()[0]
    return (result + 1) if result is not None else 1


def load_news_data(json_path: str) -> list:
    """
    Load news data from JSON file generated by scrape_mining.py
    
    Args:
        json_path: Path to the JSON file containing news articles
    
    Returns:
        list: List of articles loaded from the JSON file
    """
    try:
        with open(json_path, 'r', encoding='utf-8') as file:
            return json.load(file)
    except FileNotFoundError:
        LOGGER.info(f"File {json_path} not found.")
        return []
    except json.JSONDecodeError:
        LOGGER.info(f"Invalid JSON in {json_path}")
        return []


def prepare_news_data(articles: list) -> pd.DataFrame:
    """
    Prepare articles data for database insertion.
    Convert commodities list to comma-separated string.
    
    Args:
        articles: List of articles, each article is a dictionary
        
    Returns:
        pd.DataFrame: DataFrame containing prepared articles data
    """
    if not articles:
        return pd.DataFrame()
    
    df = pd.DataFrame(articles)
    
    # Filter out articles with missing critical data
    required_cols = ['title']
    df = df.dropna(subset=required_cols)
    
    # Remove articles with empty titles
    df = df[df['title'].str.strip() != '']
    return df


def filter_new_articles(conn: sqlite3.Connection, df:pd.DataFrame) -> pd.DataFrame: 
    """ 
    Filter out articles that already exist in the database to prevent duplicates.
    
    Args: 
        conn: SQLite connection object
        df: DataFrame containing articles to be inserted
    
    Returns:
        pd.DataFrame: DataFrame containing only new articles that are not in the database
    """
    if df.empty: 
        return df 
    
    # Get existing sources from the database
    cursor = conn.cursor()
    cursor.execute("SELECT source FROM mining_news WHERE source IS NOT NULL;")
    existing_sources = set(row[0] for row in cursor.fetchall())
    
    # Filter out articles that already exist in the database
    new_articles = df[~df['source'].isin(existing_sources)]
    LOGGER.info(f"Found {len(df)} total articles, {len(new_articles)} are new.")
    return new_articles


def insert_news_records(conn: sqlite3.Connection, df: pd.DataFrame):
    """
    Insert news records into mining_news table. 
    Check for duplicates based on source and construct a new ID for each article. 
    Only insert new articles.
    
    Args:
        conn: SQLite connection object
        df: DataFrame containing articles to be inserted
    """
    # Check if DataFrame is empty
    if df.empty:
        LOGGER.info("No valid news records to insert.")
        return

    # Filter out new articles that are not already in the database
    new_articles_df = filter_new_articles(conn, df)
    
    # If no new articles, exit early
    if new_articles_df.empty:
        LOGGER.info("No new articles to insert (all are duplicates).")
        return
    
    # Get the next available ID for insertion
    next_id = get_next_id(conn)
    
    # Assign consecutive IDs to new articles
    new_articles_df = new_articles_df.copy()
    new_articles_df['id'] = range(next_id, next_id + len(new_articles_df))

    # Prepare SQL insert statement
    insert_sql = """
    INSERT INTO mining_news (
        id, title, body, source, timestamp, commodities
    ) VALUES (
        :id, :title, :body, :source, :timestamp, :commodities
    );
    """
    
    # Convert commodities list to JSON string
    new_articles_df['commodities'] = new_articles_df['commodities'].apply(
        lambda x: json.dumps(x) if isinstance(x, list) else []
    )
    
    # Convert DataFrame to list of dictionaries for insertion
    records = new_articles_df[['id', 'title', 'body', 'source', 'timestamp', 'commodities']].to_dict(orient='records')
    
    # Count rows before insert
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM mining_news;")
    before = cur.fetchone()[0]

    with conn:
        conn.executemany(insert_sql, records)

    # Count rows after insert
    cur.execute("SELECT COUNT(*) FROM mining_news;")
    after = cur.fetchone()[0]

    inserted = after - before
    LOGGER.info(f"Inserted {inserted} new news records (duplicates ignored).")


def scrape_and_insert_news(num_pages: int, 
                           db_path: str, 
                           limit_scrape: int,
                           output_filename: Optional[str] = None):
    """
    Full pipeline:
    1. Scrape Article data from miningnews.com and coalmetal.com
       (using MiningScraper and coalmetal scraping function)
    2. Save to JSON (optional for miningnews)
    3. Prepare data for database
    4. Create table if needed
    5. Insert into SQLite
    """
    # Step 1: Scrape news data from miningnews.com
    LOGGER.info(f"Scraping {num_pages} pages of mining news...")

    scraper = MiningScraper()
    df_articles_mining_news = scraper.extract_news_pages(num_pages)
    if df_articles_mining_news.empty:
        LOGGER.info("No articles found from miningnews.com")
        return pd.DataFrame()
    
    # Scrape news data from coalmetal.com
    df_articles_coalmetal = run_coalmetal_scraping(limit_article=limit_scrape)
    if df_articles_coalmetal.empty:
        LOGGER.info("No articles found from coalmetal.com")
        return pd.DataFrame()

    LOGGER.info(f"Scraped {df_articles_mining_news.shape[0]} articles from miningnews" 
                f"Scraped {df_articles_coalmetal.shape[0]} articles from coalmetal after filtering")
    
    # Step 2: Save to JSON (optional)
    if output_filename:
        os.makedirs('insider_news/data', exist_ok=True)
        scraper.write_json(df_articles_mining_news, output_filename)
        scraper.write_json(df_articles_coalmetal, f"{output_filename}_coalmetal")
        LOGGER.info(f"Saved articles_mining_news to {output_filename}.json")
    
    # Step 3: Prepare data for coalmetal and miningnews
    df_articles_mining_news = prepare_news_data(df_articles_mining_news, df_articles_coalmetal, 
                                                is_mining_news=True)
    df_articles_coalmetal = prepare_news_data(df_articles_mining_news, df_articles_coalmetal,
                                               is_mining_news=False)

    # Step 4 & 5: Create table and insert data
    conn = get_connection(db_path)
    create_news_table(conn)
    # insert for miningnews scraping
    insert_news_records(conn, df_articles_mining_news)
    #insert for coalmetal scraping
    insert_news_records(conn, df_articles_coalmetal)

    conn.close()

def load_and_insert_news(json_path: str):
    """
    Load news data from existing JSON file and insert into database.
    
    Args: 
        json_path: Path to the JSON file containing news articles
        db_path: Path to SQLite database
    """
    LOGGER.info(f"Loading news data from {json_path}...")
    articles = load_news_data(json_path)
    
    if not articles:
        return
    
    LOGGER.info(f"Loaded {len(articles)} articles.")
    
    # Prepare data
    df = prepare_news_data(articles)
    
    if df.empty:
        LOGGER.info("No valid articles to insert after filtering.")
        return
    
    # Create table and insert data
    conn = get_connection()
    create_news_table(conn)
    insert_news_records(conn, df)
    conn.close()
    
    
def archive_old_news(days_old: int = 182, archive_path: str = 'insider_news/data/archive'): 
    """
    Delete news articles older than specified days based on their publication timestamp.
    
    Args:
        db_path: Path to SQLite database
        days_old: Delete articles older than this many days (default: 365 = 1 year)
    """
    # Create a connection to the database
    conn = get_connection()

    # Calculate cutoff date
    cutoff_date = datetime.now() - timedelta(days=days_old)
    cutoff_str = cutoff_date.strftime('%Y-%m-%d')
    
    # SQL to delete old articles
    query = """
        SELECT * FROM mining_news 
        WHERE timestamp IS NOT NULL 
        AND timestamp < ?
    """
    
    # Execute deletion
    articles_to_archive_df  = pd.read_sql_query(query, conn, params=(cutoff_str,))
    conn.close()
    
    if articles_to_archive_df .empty:
        LOGGER.info(f"No articles older than {days_old} days found to archive.")
        conn.close()
        return pd.DataFrame()
    
    # Create archive directory if it doesn't exist
    os.makedirs(archive_path, exist_ok=True)
    csv_path = os.path.join(archive_path, f'mining_news_all_archived.csv')
    
    # Check if the archive file already exists and read existing sources
    existing_archived_sources = set()
    if os.path.exists(csv_path):
        try: 
            # Read the unique 'source' column from the existing archive
            archive_df = pd.read_csv(csv_path, usecols=['source'])
            existing_archived_sources = set(archive_df['source'])
        except (pd.errors.EmptyDataError, FileNotFoundError):
            LOGGER.error(f"Archive file at {csv_path} is empty or not found. A new one will be created.")
        except Exception as e:
            LOGGER.error(f"Could not read existing archive file: {e}")
            return
    
    # Filter out articles that have already been archived
    unique_articles_to_archive_df = articles_to_archive_df[
        ~articles_to_archive_df['source'].isin(existing_archived_sources)
    ]
    
    # If no new articles to archive, log and exit
    if unique_articles_to_archive_df.empty:
        LOGGER.info("All old articles found in the database have already been archived.")
        return
    
    # Add archived date column
    unique_articles_to_archive_df = unique_articles_to_archive_df.copy()
    unique_articles_to_archive_df['archived_date'] = datetime.now().strftime('%Y-%m-%d')
    
    # Check if the file is empty to decide whether to write headers
    file_exists_and_is_not_empty = os.path.exists(csv_path) and os.path.getsize(csv_path) > 0

    # Write the unique articles to the CSV file, appending if it exists
    unique_articles_to_archive_df.to_csv(
        csv_path, 
        mode='a', 
        header=not file_exists_and_is_not_empty, 
        index=False
    )

    LOGGER.info(f"Archived {len(unique_articles_to_archive_df)} new articles to {csv_path}. Duplicates were ignored.")
    

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipeline for inserting mining news into database")
    parser.add_argument("--scrape", action="store_true", help="Scrape new data from mining.com")
    parser.add_argument("--load", type=str, help="Load data from existing JSON file")
    parser.add_argument("--pages", type=int, default=1, help="Number of pages to scrape (default: 1)")
    parser.add_argument("--db", type=str, default="db.sqlite", help="Database path (default: db.sqlite)")
    parser.add_argument("--output", type=str, help="Output filename for JSON (optional)")
    parser.add_argument("--limit-coalmetal", type=int, default=15, help="Limit number of articles to scrape coalmetal.com (default: 10)")
    parser.add_argument("--archive", action="store_true", help="Archive all mining_news records older than `--days-old` days into CSV")
    parser.add_argument("--days-old", type=int, default=182, dest="days_old", help="Archive articles older than this many days (default: 182)")
    parser.add_argument("--archive-path", type=str, default="insider_news/data/archive", dest="archive_path", help="Path to the directory for saving archived news")

    args = parser.parse_args()
    
    if args.scrape:
        scrape_and_insert_news(args.pages, args.db, args.output)
    elif args.load:
        load_and_insert_news(args.load, args.db)
    elif args.archive: 
        archive_old_news(days_old=args.days_old, archive_path=args.archive_path)
    else:
        LOGGER.info("Please specify either --scrape, --load, or --archive option.")
        LOGGER.info("Examples:")
        LOGGER.info("  python pipeline.py --scrape --pages 3 --db db.sqlite --output mining_news")
        LOGGER.info("  python pipeline.py --load insider_news/data/mining_news.json --db db.sqlite")